{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time \n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from create_financial_database import get_credentials \n",
    "from SQLite_tools import query_stock_data, check_if_close_price_exists\n",
    "from ticker_loader import load_SPY_components\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "try:\n",
    "    from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "except ModuleNotFoundError:\n",
    "    from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Output :\n",
      "2.2.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1 \n",
      "\n",
      "Actual Output :\n",
      "2.2.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "CUDA: True cuda\n",
      "\n",
      "PyTorch Devices:  NVIDIA GeForce RTX 3060\n",
      "Using GPU:  True\n",
      "DEVICE:  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Output :\")\n",
    "print(\"2.2.0+cu121\")\n",
    "print(\"CUDA available: True\")\n",
    "print(\"CUDA version: 12.1 \\n\")\n",
    "print(\"Actual Output :\")\n",
    "print(torch.__version__)  # To check the PyTorch version\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "print(\"\")\n",
    "print(\"PyTorch Devices: \", torch.cuda.get_device_name(0))\n",
    "print(\"Using GPU: \", USE_CUDA)\n",
    "print(\"DEVICE: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example chunk: ../Data/Networks_chunks/window_size_5\\chunk_1996-11-07__5.pkl\n",
      "Number of graphs in the chunk: 5\n",
      "Example Graph - index 0: Data(x=[734, 71], edge_index=[2, 1011], edge_attr=[1011], y=[734])\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the chunks\n",
    "# CHUNK_DATA_DIR = \"../Data/Networks_chunks/window_size_10\"\n",
    "# CHUNK_DATA_DIR = \"../Data/Networks_chunks/window_size_10\"\n",
    "# CHUNK_DATA_DIR = \"../Data/Networks_chunks/window_size_3\"\n",
    "CHUNK_DATA_DIR = \"../Data/Networks_chunks/window_size_5\"\n",
    "\n",
    "# Helper function to load a chunk\n",
    "def load_chunk(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# List all chunk files\n",
    "chunk_files = [os.path.join(CHUNK_DATA_DIR, f) for f in os.listdir(CHUNK_DATA_DIR) if f.endswith(\".pkl\")]\n",
    "\n",
    "# Load an example chunk\n",
    "example_chunk_path = chunk_files[150]\n",
    "print(f\"Loading example chunk: {example_chunk_path}\")\n",
    "example_chunk = load_chunk(example_chunk_path)\n",
    "\n",
    "# Inspect the first graph in the chunk\n",
    "print(f\"Number of graphs in the chunk: {len(example_chunk)}\")\n",
    "print(f\"Example Graph - index 0: {example_chunk[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[734, 71], edge_index=[2, 1011], edge_attr=[1011], y=[734])\n",
      "torch.Size([734, 71])\n",
      "734\n",
      "71\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(example_chunk[0])\n",
    "print(example_chunk[0].x.size())\n",
    "print(example_chunk[0].x.size()[0])\n",
    "print(example_chunk[0].x.size()[1])\n",
    "print(len(example_chunk))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/Networks_chunks/window_size_5\\\\chunk_1996-04-08__5.pkl',\n",
       " '../Data/Networks_chunks/window_size_5\\\\chunk_1996-04-09__5.pkl',\n",
       " '../Data/Networks_chunks/window_size_5\\\\chunk_1996-04-10__5.pkl',\n",
       " '../Data/Networks_chunks/window_size_5\\\\chunk_1996-04-11__5.pkl',\n",
       " '../Data/Networks_chunks/window_size_5\\\\chunk_1996-04-12__5.pkl',\n",
       " '../Data/Networks_chunks/window_size_5\\\\chunk_1996-04-15__5.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_chunk(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def load_chunks(file_list):\n",
    "    chunks = []\n",
    "    for file in file_list:\n",
    "        with open(file, \"rb\") as f:\n",
    "            chunk_graphs = pickle.load(f)\n",
    "        primary_target = chunk_graphs[-1].y  \n",
    "        chunks.append((chunk_graphs, primary_target))\n",
    "    return chunks\n",
    "\n",
    "        \n",
    "def prepare_ordered_splits(CHUNK_DATA_DIR, train_ratio=0.7, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Prepares train, validation, and test datasets from chunked graph data in chronological order.\n",
    "\n",
    "    Args:\n",
    "        CHUNK_DATA_DIR (str): Directory containing graph chunk files.\n",
    "        train_ratio (float): Proportion of data to use for training.\n",
    "        val_ratio (float): Proportion of data to use for validation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lists of train, validation, and test chunk files.\n",
    "    \"\"\"\n",
    "    chunk_files = [os.path.join(CHUNK_DATA_DIR, f) for f in os.listdir(CHUNK_DATA_DIR) if f.endswith(\".pkl\")]\n",
    "    chunk_files = sorted(chunk_files, key=lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    num_files = len(chunk_files)\n",
    "    train_end = int(num_files * train_ratio)\n",
    "    val_end = train_end + int(num_files * val_ratio)\n",
    "\n",
    "    train_files = chunk_files[:train_end]\n",
    "    val_files = chunk_files[train_end:val_end]\n",
    "    test_files = chunk_files[val_end:]\n",
    "\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "\n",
    "train_files, val_files, test_files = prepare_ordered_splits(CHUNK_DATA_DIR)\n",
    "\n",
    "train_files[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start Date: 1996-04-08\n",
      "Train End Date: 2016-01-07\n",
      "Validation Start Date: 2016-01-08\n",
      "Validation End Date: 2018-10-31\n",
      "Test Start Date: 2018-11-01\n",
      "Test End Date: 2024-06-28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_start_date = train_files[0].split(\"_\")[4]\n",
    "train_end_date   = train_files[-1].split(\"_\")[4]\n",
    "val_start_date   = val_files[0].split(\"_\")[4]\n",
    "val_end_date     = val_files[-1].split(\"_\")[4]\n",
    "test_start_date  = test_files[0].split(\"_\")[4]\n",
    "test_end_date    = test_files[-1].split(\"_\")[4]\n",
    "\n",
    "print(\"Train Start Date:\", train_start_date)\n",
    "print(\"Train End Date:\", train_end_date)\n",
    "print(\"Validation Start Date:\", val_start_date)\n",
    "print(\"Validation End Date:\", val_end_date)\n",
    "print(\"Test Start Date:\", test_start_date)\n",
    "print(\"Test End Date:\", test_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class A3TGCN2ChunkDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        \"\"\"\n",
    "        Dataset for A3TGCN2 with chunked graph data.\n",
    "\n",
    "        Args:\n",
    "            file_paths (list): List of paths to chunk files.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns processed data for a single chunk.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the chunk.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (X, edge_index, edge_weight, y) for the chunk.\n",
    "        \"\"\"\n",
    "        chunk_path = self.file_paths[idx]\n",
    "        chunk = load_chunk(chunk_path) \n",
    "\n",
    "        X = torch.stack([graph.x for graph in chunk], dim=1)  \n",
    "        A = chunk[-1].edge_index \n",
    "        edge_index = A\n",
    "        edge_weight = chunk[-1].edge_attr  \n",
    "        y = chunk[-1].y  \n",
    "\n",
    "        return X, edge_index, edge_weight, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_batch dtype: torch.float32, X_batch shape: torch.Size([1, 734, 71, 5])\n",
      "edge_index_batch dtype: torch.int64, edge_index_batch shape: torch.Size([2, 2])\n",
      "edge_weight_batch dtype: torch.float32, edge_weight_batch shape: torch.Size([2])\n",
      "y_batch dtype: torch.float32, y_batch shape: torch.Size([734])\n",
      "y_batch mean: 1.3746594190597534,  unique: tensor([-2., -1.,  0.,  1.,  2.])\n",
      "\n",
      "X_batch:  torch.Size([1, 734, 71])\n",
      "\n",
      "No Nan values detected in the data.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def a3tgcn2_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for batching A3TGCN2 data.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of (X, edge_index, edge_weight, y) tuples.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Batched X, edge_index, edge_weight, and y.\n",
    "    \"\"\"\n",
    "    X_batch = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)  # [B, N, T, d]\n",
    "    \n",
    "    edge_indices = []\n",
    "    edge_weights = []\n",
    "    y_batch = []\n",
    "\n",
    "    node_offset = 0  \n",
    "    for i, (X, edge_index, edge_weight, y) in enumerate(batch):\n",
    "\n",
    "        # NOTE: Edge data will be empty for this test, allowing the model only to use the node featur\n",
    "        if len(edge_indices) == 0:\n",
    "            edge_index = edge_index + node_offset\n",
    "            # print(\"edge_index: \", edge_index)\n",
    "            # print(\"edge_weight: \", edge_weight)\n",
    "            # Make a dummy edge_index and edge_weight\n",
    "            edge_index = torch.tensor([[1, 1],\n",
    "                                        [1,1]], dtype=torch.long)\n",
    "            edge_weight = torch.tensor([1.0, 1.0], dtype=torch.float)\n",
    "\n",
    "\n",
    "            edge_indices.append(edge_index)\n",
    "            edge_weights.append(edge_weight)\n",
    "\n",
    "        y_batch.append(y)\n",
    "        # node_offset += X.size(0)  # Update node offset (number of nodes in the graph)\n",
    "\n",
    "    edge_index_batch = torch.cat(edge_indices, dim=1)  # [2, num_edges]\n",
    "    edge_weight_batch = torch.cat(edge_weights)  # [num_edges]\n",
    "    y_batch = torch.cat(y_batch)  # [B * N]\n",
    "\n",
    "    # re-adjust shape such that it is [B, N, d, T]\n",
    "    X_batch = X_batch.permute(0, 1, 3, 2)\n",
    "\n",
    "    return X_batch, edge_index_batch, edge_weight_batch, y_batch\n",
    "\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "# num_nodes = 114     # Number of stocks in the graph\n",
    "# in_channels = 64    # Number of features per node\n",
    "# periods = 5        # Number of historical time steps (T)\n",
    "# out_channels = 1    # Predicting a single scalar per node (e.g., regression)\n",
    "# hidden_dim = 32     # Size of the GRU hidden state\n",
    "# batch_size = 1      # Batch size\n",
    "\n",
    "\n",
    "num_nodes    = example_chunk[0].x.size()[0]     # Number of stocks in the graph\n",
    "in_channels  = example_chunk[0].x.size()[1]     # Number of features per node\n",
    "periods      = len(example_chunk)               # Number of historical time steps (T)\n",
    "out_channels = 1                      # Predicting a single scalar per node (e.g., regression)\n",
    "hidden_dim   = 512                    # Size of the GRU hidden state\n",
    "batch_size   = 1                      # Batch size\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = A3TGCN2ChunkDataset(train_files)\n",
    "val_dataset = A3TGCN2ChunkDataset(val_files)\n",
    "test_dataset = A3TGCN2ChunkDataset(test_files)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=a3tgcn2_collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=a3tgcn2_collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=a3tgcn2_collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "nan_vals_detected = False\n",
    "\n",
    "for idx, (X_batch, edge_index_batch, edge_weight_batch, y_batch) in enumerate(train_loader):\n",
    "    if idx == 0:\n",
    "        print(\"\")\n",
    "        print(f\"X_batch dtype: {X_batch.dtype}, X_batch shape: {X_batch.shape}\")\n",
    "        print(f\"edge_index_batch dtype: {edge_index_batch.dtype}, edge_index_batch shape: {edge_index_batch.shape}\")\n",
    "        print(f\"edge_weight_batch dtype: {edge_weight_batch.dtype}, edge_weight_batch shape: {edge_weight_batch.shape}\")\n",
    "        print(f\"y_batch dtype: {y_batch.dtype}, y_batch shape: {y_batch.shape}\")\n",
    "        print(f\"y_batch mean: {y_batch.mean()},  unique: {y_batch.unique()}\")\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"X_batch: \", X_batch.mean(axis=3).shape)\n",
    "\n",
    "    if torch.isnan(X_batch).any():\n",
    "        print(\"Nan value X_batch\")\n",
    "        nan_vals_detected = True\n",
    "    if torch.isnan(edge_index_batch).any():\n",
    "        print(\"Nan value detected in edge_index_batch\")\n",
    "        nan_vals_detected = True\n",
    "    if torch.isnan(edge_weight_batch).any():\n",
    "        print(\"Nan value detected in edge_weight_batch\")\n",
    "        nan_vals_detected = True\n",
    "    if torch.isnan(y_batch).any():\n",
    "        print(\"Nan value detected in y_batch\")\n",
    "        nan_vals_detected = True\n",
    "\n",
    "print(\"\")\n",
    "if not nan_vals_detected:\n",
    "    print(\"No Nan values detected in the data.\")\n",
    "else:\n",
    "    print(\"Nan values detected in the data.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3TGCN2WithCustomOutput(\n",
      "  (base_model): A3TGCN2(\n",
      "    (_base_tgcn): TGCN2(\n",
      "      (conv_z): GCNConv(71, 512)\n",
      "      (linear_z): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (conv_r): GCNConv(71, 512)\n",
      "      (linear_r): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (conv_h): GCNConv(71, 512)\n",
      "      (linear_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class A3TGCN2WithCustomOutput(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels, num_nodes, periods, batch_size, device):\n",
    "        super(A3TGCN2WithCustomOutput, self).__init__()\n",
    "        self.device = device\n",
    "        self.base_model = A3TGCN2(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_dim,\n",
    "            periods=periods,\n",
    "            batch_size=1,\n",
    "            add_self_loops = True\n",
    "        ).to(device)\n",
    "        # self.dropout = nn.Dropout(p=0.1)  # Drop 10% of activations\n",
    "        # self.activation = nn.Tanh()\n",
    "        self.output_layer = nn.Linear(hidden_dim, out_channels).to(device)\n",
    "\n",
    "    def forward(self, X, edge_index, edge_weight, H=None):\n",
    "        # Forward pass through A3TGCN2\n",
    "        # X = self.dropout(X)\n",
    "        H_output = self.base_model(X, edge_index, edge_weight, H)\n",
    "        out = self.output_layer(H_output).squeeze(-1)  # [B * N]\n",
    "\n",
    "        return out.view(-1)\n",
    "\n",
    "\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "model = A3TGCN2WithCustomOutput(\n",
    "    in_channels=in_channels,\n",
    "    hidden_dim=hidden_dim,\n",
    "    out_channels=out_channels,\n",
    "    num_nodes=num_nodes,\n",
    "    periods=periods,\n",
    "    batch_size = batch_size,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        \"\"\"\n",
    "        Weighted MSE Loss function.\n",
    "        \n",
    "        Args:\n",
    "            weights (dict): A dictionary mapping target values to weights. This is done as some labels are more important to predict accurately than others.\n",
    "                            For this use case, we want to accurately predict the outliers (e.g., -2 and 2) more than stock with no movement (e.g., 0).\n",
    "                                      Example: {-2: 4.0, -1: 2.0, 0: 1.0, 1: 2.0, 2: 4.0}.\n",
    "                            The Default is currently all 1's, as i want to the option to test the model without the weights to see if it improves the model.\n",
    "        \"\"\"\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "        self.weights = weights if weights else {-2: 1, -1: 1.0, 0: 1.0, 1: 1.0, 2: 1.0}\n",
    "        self.base_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "        \n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted values from the model. Shape: [batch_size, ...].\n",
    "            targets (torch.Tensor): Ground truth values. Shape: [batch_size, ...].\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Computed loss value.\n",
    "        \"\"\"\n",
    "        # Get the weights for each target\n",
    "        device = predictions.device\n",
    "        target_weights = torch.tensor([self.weights[int(target.item())] for target in targets], device=device)\n",
    "\n",
    "        # Compute the base MSE loss\n",
    "        mse_loss = self.base_loss(predictions, targets)\n",
    "\n",
    "        # Encourage higher predictions:\n",
    "        # I had a problem with the model predicting 0's, followed by only positive values between 0.2 and 0.6.\n",
    "        # This was due to the model being penalized equally for false positives and false negatives.\n",
    "        # This penalty system encourages the model to predict higher values for targets `1` and `2`, and lower values for target `-2`.\n",
    "\n",
    "        too_low_prediction_penalty = torch.zeros_like(predictions)\n",
    "        too_high_prediction_penalty = torch.zeros_like(predictions)\n",
    "\n",
    "        # Apply false-negative penalty for targets `1` or `2` (encouraging high predictions)\n",
    "        high_target_mask = (targets >= 1) \n",
    "        too_low_prediction_penalty[high_target_mask] = (predictions[high_target_mask] < targets[high_target_mask]).float() * 1.5\n",
    "        too_high_prediction_penalty[high_target_mask] = (predictions[high_target_mask] > targets[high_target_mask]).float() * 0.75\n",
    "\n",
    "        # Apply false-positive penalty for targets `-2` (encouraging low predictions)\n",
    "        low_target_mask = (targets == -2)\n",
    "        too_low_prediction_penalty[low_target_mask] = (predictions[low_target_mask] > targets[low_target_mask]).float() * 1.5\n",
    "        too_high_prediction_penalty[low_target_mask] = (predictions[low_target_mask] < targets[low_target_mask]).float() * 0.75\n",
    "\n",
    "        #The Combined penalties are subtracted from eachother to balance the loss, ensuring that the model is not encouraged to predict high values for all targets.\n",
    "        penalties = 1 + too_low_prediction_penalty - too_high_prediction_penalty\n",
    "\n",
    "        weighted_loss = mse_loss * target_weights\n",
    "\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "# # Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.00001) \n",
    "# weights = {-2: 4.0, -1: 2.0, 0: 1.0, 1: 2.0, 2: 4.0}\n",
    "weights = {-2: 3.0, -1: 2.0, 0: 0.5, 1: 2.0, 2: 5.0}\n",
    "criterion = WeightedMSELoss(weights=weights)\n",
    "criterion_unweighted = WeightedMSELoss(weights=None)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_loss(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Calculates the test loss for the given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (nn.Module): Loss function to use.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        float: The average test loss.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for X_batch, edge_index_batch, edge_weight_batch, y_batch in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X_batch, edge_index_batch, edge_weight_batch, y_batch = (\n",
    "                X_batch.to(device),\n",
    "                edge_index_batch.to(device),\n",
    "                edge_weight_batch.to(device),\n",
    "                y_batch.to(device),\n",
    "            )\n",
    "\n",
    "            # Forward pass through the model\n",
    "            predictions = model(X_batch, edge_index_batch, edge_weight_batch)\n",
    "            # predictions = torch.nan_to_num(predictions, nan=0.0)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = total_loss / num_batches \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model at the last state from the checkpoint IF the notebook crashed or was stopped for some reason.\n",
    "\n",
    "# model_name = \"A3TGCN2_2\"\n",
    "# checkpoint = torch.load(f\"../Data/Models/{model_name}.pt\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# best_val_loss = checkpoint['best_val_loss']\n",
    "# model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 in progress...: 100%|██████████| 4973/4973 [10:30<00:00,  7.89it/s]\n",
      "Epoch 2/500 in progress...:   0%|          | 1/4973 [00:00<11:23,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Training Loss Weighted MSE: 4.3542    Validation Loss Weighted MSE: 3.7209 | Unweighted MSE: 1.2097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/500 in progress...: 100%|██████████| 4973/4973 [10:49<00:00,  7.66it/s]\n",
      "Epoch 3/500 in progress...:   0%|          | 1/4973 [00:00<11:36,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, Training Loss Weighted MSE: 4.3346    Validation Loss Weighted MSE: 3.7261 | Unweighted MSE: 1.2144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/500 in progress...: 100%|██████████| 4973/4973 [10:44<00:00,  7.71it/s]\n",
      "Epoch 4/500 in progress...:   0%|          | 1/4973 [00:00<11:01,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500, Training Loss Weighted MSE: 4.3306    Validation Loss Weighted MSE: 3.7310 | Unweighted MSE: 1.2416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/500 in progress...: 100%|██████████| 4973/4973 [10:48<00:00,  7.67it/s]\n",
      "Epoch 5/500 in progress...:   0%|          | 1/4973 [00:00<11:08,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500, Training Loss Weighted MSE: 4.3295    Validation Loss Weighted MSE: 3.7275 | Unweighted MSE: 1.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/500 in progress...: 100%|██████████| 4973/4973 [10:44<00:00,  7.72it/s]\n",
      "Epoch 6/500 in progress...:   0%|          | 1/4973 [00:00<11:14,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500, Training Loss Weighted MSE: 4.3274    Validation Loss Weighted MSE: 3.7255 | Unweighted MSE: 1.2239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/500 in progress...: 100%|██████████| 4973/4973 [10:19<00:00,  8.02it/s]\n",
      "Epoch 7/500 in progress...:   0%|          | 1/4973 [00:00<10:10,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500, Training Loss Weighted MSE: 4.3272    Validation Loss Weighted MSE: 3.7275 | Unweighted MSE: 1.2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/500 in progress...: 100%|██████████| 4973/4973 [10:19<00:00,  8.03it/s]\n",
      "Epoch 8/500 in progress...:   0%|          | 1/4973 [00:00<10:47,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500, Training Loss Weighted MSE: 4.3239    Validation Loss Weighted MSE: 3.7316 | Unweighted MSE: 1.2181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/500 in progress...: 100%|██████████| 4973/4973 [10:26<00:00,  7.93it/s]\n",
      "Epoch 9/500 in progress...:   0%|          | 1/4973 [00:00<11:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500, Training Loss Weighted MSE: 4.3239    Validation Loss Weighted MSE: 3.7450 | Unweighted MSE: 1.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/500 in progress...: 100%|██████████| 4973/4973 [11:17<00:00,  7.34it/s]\n",
      "Epoch 10/500 in progress...:   0%|          | 1/4973 [00:00<12:16,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500, Training Loss Weighted MSE: 4.3227    Validation Loss Weighted MSE: 3.7400 | Unweighted MSE: 1.2214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/500 in progress...: 100%|██████████| 4973/4973 [11:21<00:00,  7.30it/s]\n",
      "Epoch 11/500 in progress...:   0%|          | 1/4973 [00:00<11:45,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500, Training Loss Weighted MSE: 4.3205    Validation Loss Weighted MSE: 3.7468 | Unweighted MSE: 1.2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/500 in progress...: 100%|██████████| 4973/4973 [10:49<00:00,  7.66it/s]\n",
      "Epoch 12/500 in progress...:   0%|          | 1/4973 [00:00<12:59,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500, Training Loss Weighted MSE: 4.3217    Validation Loss Weighted MSE: 3.7264 | Unweighted MSE: 1.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/500 in progress...: 100%|██████████| 4973/4973 [10:36<00:00,  7.82it/s]\n",
      "Epoch 13/500 in progress...:   0%|          | 1/4973 [00:00<11:05,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500, Training Loss Weighted MSE: 4.3179    Validation Loss Weighted MSE: 3.7359 | Unweighted MSE: 1.2195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/500 in progress...: 100%|██████████| 4973/4973 [11:11<00:00,  7.41it/s]\n",
      "Epoch 14/500 in progress...:   0%|          | 1/4973 [00:00<11:17,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500, Training Loss Weighted MSE: 4.3194    Validation Loss Weighted MSE: 3.7321 | Unweighted MSE: 1.2392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/500 in progress...: 100%|██████████| 4973/4973 [11:26<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping: validation loss did not improve for 5 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 500\n",
    "force_epochs = 10\n",
    "stop_training = False\n",
    "patience = 5\n",
    "cur_patience = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "model_name = \"A3TGCN2_7\"\n",
    "\n",
    "train_losses = []\n",
    "train_unweighted_losses = []\n",
    "val_losses = []\n",
    "val_unweighted_losses = []\n",
    "\n",
    "if not os.path.exists(f\"../Data/Models/\"):\n",
    "    os.makedirs(f\"../Data/Models/\")\n",
    "# elif os.path.exists(f\"../Data/Models/{model_name}.pt\"):\n",
    "#     print(f\"Model {model_name} already exists. Do you want to overwrite it? (y/n)\")\n",
    "#     response = input()\n",
    "#     if response.lower() != \"y\":\n",
    "#         print(\"Exiting training loop...\")\n",
    "#         sys.exit()\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    total_loss = 0\n",
    "    total_unweighted_loss = 0\n",
    "    model.train()\n",
    "    for X_batch, edge_index_batch, edge_weight_batch, y_batch in tqdm(train_loader, total = len(train_loader), desc=f\"Epoch {epoch + 1}/{epochs} in progress...\"):\n",
    "        # Move to DEVICE\n",
    "        # X_batch = clip_outliers(X_batch)\n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "        X_batch, edge_index_batch, edge_weight_batch, y_batch = (\n",
    "            X_batch.to(DEVICE), \n",
    "            edge_index_batch.to(DEVICE), \n",
    "            edge_weight_batch.to(DEVICE),  \n",
    "            y_batch.to(DEVICE)\n",
    "        )\n",
    "\n",
    "        #[B, N, d, T]\n",
    "        batch_size, num_nodes, num_features, time_steps = X_batch.size()\n",
    "        if epoch == 0:\n",
    "            assert not torch.isnan(edge_weight_batch).any(), \"Edge weights contain NaN values!\"\n",
    "            assert not torch.isinf(edge_weight_batch).any(), \"Edge weights contain Inf values!\"\n",
    "            assert edge_index_batch.dim() == 2 and edge_index_batch.size(0) == 2, \"edge_index must have shape [2, num_edges]\"\n",
    "            assert edge_index_batch.max() < num_nodes, \"edge_index contains indices outside the valid node range\"\n",
    "            assert edge_index_batch.min() >= 0, \"edge_index contains negative indices\"\n",
    "            assert edge_weight_batch.size(0) == edge_index_batch.size(1), \"edge_weight size must match the number of edges\"\n",
    "\n",
    "        # Add self-loops to edge indices\n",
    "        # edge_index_batch, edge_weight_batch = add_remaining_self_loops(\n",
    "        #     edge_index_batch, edge_attr=edge_weight_batch, fill_value=1.0\n",
    "        # )\n",
    "\n",
    "        if epoch == 0:\n",
    "            assert not torch.isnan(X_batch).any(), \"Input features contain NaN values!\"\n",
    "            assert not torch.isinf(X_batch).any(), \"Input features contain Inf values!\"\n",
    "            assert not torch.isnan(edge_weight_batch).any(), \"Edge weights contain NaN values!\"\n",
    "            assert not torch.isinf(edge_weight_batch).any(), \"Edge weights contain Inf values!\"\n",
    "\n",
    "        # Forward pass through A3TGCN2\n",
    "        predictions = model(X_batch, edge_index_batch, edge_weight_batch)#, H)\n",
    "        # predictions = torch.nan_to_num(predictions, nan=0.0)\n",
    "\n",
    "        if torch.isnan(predictions).any():\n",
    "            print(\"predictions\", predictions)\n",
    "            # replace NaN values with 0\n",
    "\n",
    "        # if all predictions are nan, break\n",
    "        if torch.isnan(predictions).all():\n",
    "            print(\"All predictions are NaN\")\n",
    "            print(\"predictions: \", predictions)\n",
    "            print(\"y_batch: \", y_batch)\n",
    "            stop_training = True\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clipping gradients in an attempt to avoid Nan/Inf values\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"loss: \", loss)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute validation loss\n",
    "    model.eval()\n",
    "    val_loss = calculate_test_loss(model, val_loader, criterion, DEVICE)\n",
    "    val_unweighted_loss = calculate_test_loss(model, val_loader, criterion_unweighted, DEVICE)\n",
    "    # train_unweighted_loss = calculate_test_loss(model, train_loader, criterion_unweighted, DEVICE)\n",
    "    total_loss = total_loss / len(train_loader)\n",
    "\n",
    "    train_losses.append(total_loss)\n",
    "    # train_unweighted_losses.append(train_unweighted_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_unweighted_losses.append(val_unweighted_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    force_epochs-=1\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        cur_patience = 0\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, f\"../Data/Models/{model_name}.pt\")\n",
    "    else:\n",
    "        if force_epochs <= 0:\n",
    "            cur_patience += 1\n",
    "            if cur_patience == patience:\n",
    "                print(f\"Early stopping: validation loss did not improve for {patience} epochs\")\n",
    "                break\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}/{epochs}, Training Loss Weighted MSE: {total_loss:.4f} | Unweighted MSE: {train_unweighted_loss:.4f}    Validation Loss Weighted MSE: {val_loss:.4f} | Unweighted MSE: {val_unweighted_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss Weighted MSE: {total_loss:.4f}    Validation Loss Weighted MSE: {val_loss:.4f} | Unweighted MSE: {val_unweighted_loss:.4f}\")\n",
    "    if stop_training:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A3TGCN2WithCustomOutput(\n",
       "  (base_model): A3TGCN2(\n",
       "    (_base_tgcn): TGCN2(\n",
       "      (conv_z): GCNConv(71, 512)\n",
       "      (linear_z): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (conv_r): GCNConv(71, 512)\n",
       "      (linear_r): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (conv_h): GCNConv(71, 512)\n",
       "      (linear_h): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = \"A3TGCN2_5\"\n",
    "# checkpoint = torch.load(f\"../Data/Models/{model_name}.pt\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# best_val_loss = checkpoint['best_val_loss']\n",
    "# model = model.to(DEVICE)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(date_of_predictions):  1422\n",
      "date_of_predictions[0]:  2018-11-01\n",
      "\n",
      "len(predictions):  1422\n",
      "predictions[0]:  [ 0.18713504  0.26728505  0.33683348  0.23380937 -0.02067674  0.43942797\n",
      "  0.14771695  0.1693549   0.29873222  0.07086572  0.26728505  0.25873852\n",
      "  0.17155436  0.40968335  0.04240787  0.04857202  0.15908572  0.32416096\n",
      "  0.19801128  0.06003696  0.05724679  0.01846218 -0.00455145  0.18727724\n",
      "  0.1731407   0.08695132  0.27933028  0.06702836  0.26795694  0.10675919\n",
      "  0.22364448  0.26728505  0.10188337 -0.06394638  0.4403374   0.19050533\n",
      "  0.23495775  0.31654358  0.20709878  0.29873222 -0.42290804  0.21678388\n",
      "  0.3579213   0.18909512  0.2052049   0.07013295  0.27693275  0.29873222\n",
      "  0.26728505  0.30708897  0.29873222  0.27942348  0.1914568   0.12074582\n",
      "  0.32461566  0.29680824  0.22394052  0.20393027  0.21460499  0.2557029\n",
      "  0.08885728  0.08922023  0.27503622  0.23647043  0.27933028  0.19178775\n",
      "  0.16422468  0.16215119  0.03340145  0.26728505  0.10050316  0.02050108\n",
      "  0.27933028  0.10079332  0.27933028  0.53525776  0.2329571   0.0256345\n",
      "  0.29873222  0.38705397  0.29873222  0.05065065  0.29873222  0.18329301\n",
      "  0.29873222  0.27933028  0.26728505  0.28299895  0.19717437  0.23647043\n",
      "  0.27933028  0.24688329  0.06540444  0.23647043  0.25813285  0.30032134\n",
      "  0.19629161  0.42037192  0.30513734  0.27933028  0.17971791  0.26728505\n",
      "  0.15943442  0.36006054  0.25873852  0.1746555   0.26728505  0.30187023\n",
      "  0.32617682  0.25873852  0.24761012 -0.00085958  0.19360918  0.35275224\n",
      "  0.29873222  0.27933028  0.27933028  0.08883573  0.16903189  0.2553264\n",
      " -0.00613457  0.14735015  0.09949419  0.26728505  0.23647043  0.06734677\n",
      "  0.29873222  0.08281114  0.23647043  0.35342723  0.30187023  0.27503622\n",
      "  0.19178775  0.28771323  0.04354623 -0.08205851  0.13242428  0.03126562\n",
      "  0.23376335 -0.01919361  0.17143635  0.30187023  0.14685851  0.2567746\n",
      "  0.27503622  0.18390337  0.18434754 -0.03621289 -0.03485415  0.14957368\n",
      "  0.08343916  0.10226072  0.14808859  0.01007395  0.30513734  0.11836233\n",
      "  0.02467925  0.17875876  0.216019    0.19720457  0.30032134  0.16253263\n",
      "  0.07137536  0.30187023  0.17171659  0.31888244  0.20425574  0.22093636\n",
      "  0.30032134  0.28307787  0.30187023  0.11385581  0.22093636  0.22093636\n",
      "  0.26728505  0.06554523  0.16956787  0.29873222  0.04744202  0.30032134\n",
      "  0.30513734  0.25884682  0.27503622  0.24022627  0.26728505  0.14335918\n",
      "  0.11522025  0.14788958  0.29873222  0.07640885  0.10126936  0.30187023\n",
      "  0.27503622  0.26728505  0.29873222  0.0670234   0.29873222  0.26728505\n",
      "  0.18350989  0.13165288  0.19626498  0.29534104  0.15990272 -0.03458869\n",
      "  0.13778572  0.15461886 -0.0626197   0.28279766  0.10959896  0.27933028\n",
      "  0.30513734  0.22093636  0.24403033  0.27503622  0.29873222  0.24447203\n",
      "  0.20578322  0.04517851  0.03049861  0.1176575   0.18719715  0.00881485\n",
      "  0.081609    0.30032134  0.5187742   0.03689524  0.16062313  0.0572824\n",
      " -0.13081335  0.25873852 -0.02033506  0.23727606  0.30032134  0.2212063\n",
      "  0.23248631  0.30032134  0.30187023  0.2772866   0.30513734  0.30187023\n",
      "  0.27984527  0.11811888 -0.34084666  0.004983    0.04574228  0.08594474\n",
      "  0.26728505  0.112556    0.21186538  0.05446335  0.29873222  0.26728505\n",
      "  0.2469777   0.2304042   0.03319326  0.22129898  0.2338268   0.02878548\n",
      "  0.4283393   0.30513734  0.21422118  0.12023177  0.10670327  0.27933028\n",
      "  0.22093636  0.08333109  0.25873852  0.03932242 -0.01397074  0.3615566\n",
      "  0.25873852  0.30187023  0.30187023  0.22521034  0.1815786   0.29173368\n",
      "  0.03176665  0.06705941  0.33945352  0.2697242   0.22085959  0.11329094\n",
      "  0.13005495  0.25873852  0.25873852  0.29873222  0.03489945  0.03519392\n",
      "  0.0840885   0.30187023  0.11553493  0.27494216  0.2553264   0.1735026\n",
      "  0.11242336  0.30187023  0.41069326  0.30032134  0.30187023  0.19178775\n",
      "  0.26728505  0.23647043  0.29549128  0.04566559  0.25873852  0.06362043\n",
      "  0.15906061  0.29873222  0.27933028  0.25873852  0.19577487  0.18420823\n",
      "  0.20639804  0.2873258   0.31787965  0.27503622  0.22074746  0.0724019\n",
      "  0.31175405  0.3792575   0.26083738  0.26728505  0.30173597  0.10017931\n",
      "  0.2616713   0.13104098  0.23624876  0.2500505   0.11702013  0.32213494\n",
      "  0.10053031  0.22123474  0.10275725  0.13159221  0.35277957  0.08155213\n",
      "  0.25004473  0.21239033 -0.07511328  0.01718155  0.21930534  0.14987996\n",
      "  0.00223778  0.27933028  0.23917018  0.27933028  0.2553264   0.46106827\n",
      "  0.09621416  0.42187297  0.27933028  0.04576575  0.29873222  0.21337716\n",
      "  0.35888672  0.09896794  0.2668516   0.22093636  0.29722595  0.2477776\n",
      "  0.32378873  0.15123037  0.25061175  0.25346923  0.14331156  0.3364329\n",
      "  0.27933028  0.21702245  0.16956174  0.27933028  0.15350054  0.30187023\n",
      "  0.14948858  0.29873222  0.22672904  0.1223371   0.30187023  0.06850629\n",
      "  0.16969983  0.26728505  0.18511146  0.28663358  0.20880952  0.26728505\n",
      "  0.29873222  0.23647043  0.03348238  0.30187023  0.03889104  0.19022584\n",
      "  0.25873852  0.29251283  0.17870037  0.05528544  0.20166886 -0.0653037\n",
      "  0.26728505  0.4056291   0.30970645  0.23647043  0.11035597  0.30187023\n",
      "  0.21534413  0.2820973   0.05290048  0.27933028  0.27503622  0.16994202\n",
      "  0.30319095  0.25873852  0.32972828  0.2481586   0.00792548  0.3135056\n",
      "  0.27933028  0.17325307  0.22093636  0.29873222  0.29873222  0.2553264\n",
      "  0.13823438  0.29873222  0.23647043  0.22493501  0.2553264   0.42775774\n",
      "  0.2968522   0.09973371  0.2424652   0.19691928  0.17734069  0.25906613\n",
      "  0.25873852  0.07249355  0.00918565  0.14047414  0.30828536  0.30032134\n",
      " -0.01186164  0.2553264   0.16308631  0.00660641  0.2553264   0.01042008\n",
      " -0.10480578  0.26728505 -0.03855405  0.25873852  0.19104254  0.04585185\n",
      "  0.14232141  0.2843772   0.05890355  0.30032134 -0.0350479   0.27740455\n",
      "  0.30187023 -0.04615783  0.30032134  0.2345851   0.04678165  0.39199173\n",
      "  0.12832445  0.19975398  0.23203075  0.2553264   0.38710365  0.25873852\n",
      "  0.27933028 -0.00158184  0.30513734  0.30187023  0.27933028  0.25873852\n",
      "  0.16729066  0.30513734  0.30513734  0.02542546  0.08019811  0.27933028\n",
      "  0.0280705   0.01083499  0.17127545  0.07479832  0.10617264  0.15521468\n",
      "  0.30187023  0.11263207  0.24706984  0.16063723  0.30187023  0.00449289\n",
      "  0.16854173  0.19483554  0.13666904  0.12710007 -0.13784638  0.29873222\n",
      "  0.07855962  0.19826953  0.19389646  0.30187023  0.2553264  -0.02636869\n",
      "  0.27933028  0.29873222  0.30032134  0.29873222  0.21333207  0.01514221\n",
      "  0.30187023  0.13728304  0.09286202  0.29932284  0.27933028  0.33995566\n",
      "  0.30187023  0.2553264   0.30187023  0.18531634  0.23505424  0.27933028\n",
      "  0.17369822  0.01643726  0.22093636  0.30032134  0.07449801  0.29873222\n",
      "  0.05784114  0.11370474  0.16384576  0.07648752  0.24696732  0.18009123\n",
      "  0.3754364   0.35465583  0.28834838  0.22590268  0.08186315  0.11381849\n",
      "  0.19216487  0.04818544  0.30032134  0.27933028  0.27408552 -0.01288475\n",
      "  0.12912497  0.15873307  0.0747896   0.28677386  0.30187023  0.26141202\n",
      "  0.19712836  0.06819157  0.22216988  0.2937035   0.26728505  0.3367717\n",
      "  0.30513734  0.18636984  0.27933028  0.26728505  0.12004138  0.26728505\n",
      "  0.1082541   0.284831    0.13695325  0.2508187   0.30513734 -0.04279294\n",
      "  0.2339207   0.14983821  0.21374393  0.16306895  0.30325535  0.15404664\n",
      "  0.30513734  0.00135924  0.27933028  0.30032134  0.30187023  0.07525599\n",
      "  0.14781511  0.11579196 -0.0467726   0.29873222  0.30187023  0.18256676\n",
      "  0.0799588   0.29873222  0.25873852  0.0673379   0.09720764  0.3261598\n",
      "  0.15959093  0.25873852  0.30187023  0.30450863  0.26728505  0.29778695\n",
      "  0.27933028  0.25873852  0.06731717  0.30032134 -0.00264162  0.10540412\n",
      "  0.18700862  0.10089304  0.30032134  0.27503622  0.26728505  0.0766971\n",
      "  0.0944213   0.29971573  0.30513734  0.15048352  0.15904443  0.30513734\n",
      "  0.23554933  0.17133717  0.28326094 -0.01575529  0.14715385  0.30187023\n",
      "  0.28147182  0.30187023  0.30032134  0.36509174  0.30187023  0.27933028\n",
      "  0.25873852  0.30032134  0.2553264   0.19858006  0.30032134  0.29154336\n",
      "  0.06820026  0.27933028  0.11094657  0.22596478  0.2553264   0.32951045\n",
      "  0.30513734  0.18250436  0.30187023  0.2397093   0.14497337  0.21172151\n",
      "  0.29873222  0.2047596   0.2905865   0.27933028  0.08023836  0.29873222\n",
      "  0.23427697  0.26728505  0.11770789  0.21021554  0.30187023  0.43392086\n",
      "  0.48483548  0.2616871   0.30187023  0.17765433  0.12374713  0.30187023\n",
      "  0.1851815   0.23647043  0.10010426  0.28130782  0.1432322   0.14954941\n",
      "  0.29873222  0.05797016  0.1300013   0.22676516  0.19073775  0.1706458\n",
      "  0.14838877  0.03779675  0.2553264   0.30187023  0.22093636  0.32596186\n",
      "  0.27933028  0.02847465  0.15182176  0.30187023  0.20574148  0.26916456\n",
      "  0.12141503  0.19178775 -0.0223816   0.30032134  0.06512713  0.27933028\n",
      "  0.15300298  0.11644578  0.2553264   0.426786   -0.1716376   0.1368716\n",
      "  0.11321647  0.29873222 -0.03097352  0.26728505  0.27606106  0.12633385\n",
      "  0.00828432 -0.03250701  0.20672914  0.27933028  0.30513734  0.25873852\n",
      "  0.25411916  0.30032134  0.25873852  0.20885661  0.2240468   0.01826377\n",
      "  0.27503622  0.21518861  0.02323807  0.09428646  0.1804817   0.09874916\n",
      "  0.1606451   0.24752502  0.3253207   0.13198361  0.32503384  0.30187023\n",
      "  0.17392543  0.23032984]\n",
      "\n",
      "len(ground_truth):  1422\n",
      "ground_truth[0]:  [ 1.  0.  1.  2. -2.  2. -1.  2.  0.  1. -1. -1.  1.  1.  1.  0.  2.  1.\n",
      "  1.  1.  1.  2.  1.  2.  1.  1.  1.  2.  2.  1.  0.  0.  0.  2.  1.  1.\n",
      "  1.  2. -1.  2.  2.  1.  0.  1.  0.  1.  2. -1.  0.  1.  2.  2.  2.  2.\n",
      " -1. -1.  2.  1.  1.  0.  1. -1.  2.  2.  0.  1. -2.  1.  1.  2.  0.  0.\n",
      " -2.  1. -1.  2.  1.  1.  1.  0.  0.  1.  2.  0.  0.  0.  0.  1.  1.  0.\n",
      "  1.  2.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0. -2.  1.  1. -1. -1.\n",
      " -1.  2.  1.  1. -1.  2.  2.  2.  0.  2.  2. -2.  0.  1. -1.  1.  1.  1.\n",
      "  2.  0.  0.  1.  1.  2.  0.  0.  0.  2.  1.  0.  0.  0.  1.  2.  1.  0.\n",
      " -1.  1.  1.  0.  1.  0.  1.  1.  2.  1. -1.  1.  1.  1.  1. -1. -1.  1.\n",
      " -2.  1.  0. -2.  0.  1.  2.  1.  0.  2.  0.  1.  0.  1.  0.  0.  1. -2.\n",
      "  1.  1.  0.  1.  0.  2.  2. -2.  1.  0.  1.  1.  1. -1.  2.  1.  1. -2.\n",
      "  0.  1.  1. -2.  1.  0.  1.  1.  0.  2.  1.  0. -2.  1.  1.  0.  1.  1.\n",
      "  2.  1.  1.  1.  0.  1. -2.  2.  0.  0.  1.  1. -1.  0.  1.  0.  2.  1.\n",
      "  0. -2.  2. -1.  2.  1.  0.  1.  0.  1.  1.  0. -1.  2.  0.  1.  2.  0.\n",
      "  1.  1.  1.  2. -1.  0.  1. -2.  2. -1. -2.  1.  1. -1.  1.  1.  1.  2.\n",
      "  0.  2.  0.  1.  1.  2.  1.  2.  1.  2.  2.  1.  1. -1. -1. -2.  0.  0.\n",
      "  1.  1. -1. -2. -2.  1.  1. -2. -2.  0.  2.  0.  0.  2.  1.  0.  1.  1.\n",
      "  0. -1.  1.  2.  0.  0.  1. -1.  1.  0. -1.  1.  0.  1.  1.  0.  0.  1.\n",
      "  1.  1.  1.  1.  0. -1.  0.  1.  2.  1.  0.  1.  1.  2.  1.  0.  1. -1.\n",
      "  0.  1.  2. -1. -2.  2.  2.  1.  1.  0. -2.  2.  0.  1.  2.  0.  1.  1.\n",
      "  0.  1.  2.  1.  1.  1.  2.  1. -1.  2. -1.  0.  1. -1.  2.  1. -2.  1.\n",
      "  1.  0.  1.  1.  1.  0. -1.  2.  1.  2. -2.  1.  0.  1.  0.  1. -1.  1.\n",
      " -2.  2.  2.  0.  2.  1.  0. -2.  1.  0.  1.  1.  1.  1.  1.  2.  1.  1.\n",
      "  1.  0.  0. -2. -1.  1.  1. -2.  2.  1.  2.  2.  1.  1. -1. -1. -1. -1.\n",
      " -1.  2.  2.  1.  1.  2.  1.  2.  1.  2. -1. -1. -1.  0.  1. -1.  2.  1.\n",
      "  2.  0.  1.  2.  2. -2.  1.  1.  0. -1.  0.  1.  2.  1.  1. -2.  2.  1.\n",
      " -1.  0. -1.  0. -2.  2.  1. -1.  1.  2.  2.  1.  1.  0.  0.  1.  0. -2.\n",
      "  2.  0.  1. -1.  0.  0.  1.  1.  2.  2. -2. -1.  2.  2.  2.  0. -1.  1.\n",
      "  1.  2.  0.  0.  0.  1.  0.  1.  1.  2.  0.  2.  0. -1.  0.  2.  0.  1.\n",
      "  1.  0. -1.  2.  0. -2.  1.  0.  1.  1.  2.  2. -1.  1.  2.  1.  0.  1.\n",
      "  2.  2.  0.  1.  1.  1. -2.  1.  0. -1.  0.  0.  2. -1.  1.  2.  0. -2.\n",
      "  0. -2.  0.  0.  0.  1.  0.  1.  1.  2. -2.  1.  0.  1.  0.  2.  1.  2.\n",
      "  1.  1.  1.  2.  1.  1.  2.  0.  2.  1. -2.  2.  1.  2.  0.  1.  0.  0.\n",
      "  1.  1. -2.  1.  0.  1.  1.  0.  1.  0.  1.  1. -2.  2.  2.  0.  0.  2.\n",
      "  2.  1.  2.  1. -2.  1. -2.  1. -2.  1. -1.  1.  1.  0.  1.  1.  0. -2.\n",
      "  1.  0.  1.  1. -1.  0.  0.  1.  1.  2.  0.  0.  0.  2.  0.  1.  1.  1.\n",
      "  1. -1.  0.  2. -2.  1. -1.  0.  0.  1. -1. -1. -1.  2.  0.  1.  2.  0.\n",
      "  2.  0.  2.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1. -1. -1. -2.  0. -2.\n",
      "  0.  2.  1.  0.  2.  2.  1.  2.  1.  2.  1.  1.  1.  1.  1.  0.  0.  1.\n",
      "  1. -1.  0.  0.  1.  2.  1.  0.  1.  1. -1.  1.  1.  2.  2.  2.  0. -2.\n",
      "  1.  2.  1.  0.  0.  0.  1.  1.  1.  1.  1.  2.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Once the model is done training, we can evaluate it on the test set, save the model weights, and save the predictions together with the ground truth values for further analysis.\n",
    "\n",
    "def model_evaluation(model, test_loader, test_files, model_name):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set.  \n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Predictions and ground truth values.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    val_predictions = []\n",
    "    ground_truth = []\n",
    "    val_ground_truth = []\n",
    "    total_test_loss = 0\n",
    "    total_val_loss = 0\n",
    "    total_unweighted_test_loss = 0\n",
    "    total_unweighted_val_loss = 0\n",
    "\n",
    "    # ['../Data/Networks_chunks/window_size_5\\\\chunk_2014-04-09__5.pkl',\n",
    "    date_of_predictions = [x.split(\"chunk_\")[1].split(\"__\")[0] for x in test_files]\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for X_batch, edge_index_batch, edge_weight_batch, y_batch in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X_batch, edge_index_batch, edge_weight_batch, y_batch = (\n",
    "                X_batch.to(DEVICE),\n",
    "                edge_index_batch.to(DEVICE),\n",
    "                edge_weight_batch.to(DEVICE),\n",
    "                y_batch.to(DEVICE),\n",
    "            )\n",
    "\n",
    "            # Forward pass through the model\n",
    "            batch_predictions = model(X_batch, edge_index_batch, edge_weight_batch)\n",
    "            predictions.append(batch_predictions.cpu().numpy())\n",
    "            ground_truth.append(y_batch.cpu().numpy())\n",
    "            total_test_loss += criterion(batch_predictions, y_batch).item()\n",
    "            total_unweighted_test_loss += criterion_unweighted(batch_predictions, y_batch).item()\n",
    "\n",
    "        for X_batch, edge_index_batch, edge_weight_batch, y_batch in val_loader:\n",
    "            # Move data to the specified device\n",
    "            X_batch, edge_index_batch, edge_weight_batch, y_batch = (\n",
    "                X_batch.to(DEVICE),\n",
    "                edge_index_batch.to(DEVICE),\n",
    "                edge_weight_batch.to(DEVICE),\n",
    "                y_batch.to(DEVICE),\n",
    "            )\n",
    "\n",
    "            # Forward pass through the model\n",
    "            val_batch_predictions = model(X_batch, edge_index_batch, edge_weight_batch)\n",
    "            val_predictions.append(val_batch_predictions.cpu().numpy())\n",
    "            val_ground_truth.append(y_batch.cpu().numpy())\n",
    "            total_val_loss += criterion(val_batch_predictions, y_batch).item()\n",
    "            total_unweighted_val_loss += criterion_unweighted(val_batch_predictions, y_batch).item()\n",
    "\n",
    "    # Compute average test loss\n",
    "    test_loss = total_test_loss / len(test_loader)\n",
    "    test_unweighted_loss = total_unweighted_test_loss / len(test_loader)\n",
    "\n",
    "    print(\"len(date_of_predictions): \", len(date_of_predictions))\n",
    "    print(\"date_of_predictions[0]: \", date_of_predictions[0])\n",
    "    print(\"\")\n",
    "    print(\"len(predictions): \", len(predictions))\n",
    "    print(\"predictions[0]: \", predictions[0])\n",
    "    print(\"\")\n",
    "    print(\"len(ground_truth): \", len(ground_truth))\n",
    "    print(\"ground_truth[0]: \", ground_truth[0])\n",
    "\n",
    "\n",
    "    # Merge results into a single dataframe\n",
    "    results_dict ={\n",
    "            \"Date\": date_of_predictions,\n",
    "            \"Prediction\": predictions,\n",
    "            \"Ground Truth\": ground_truth,\n",
    "            \"Val_Prediction\": val_predictions,\n",
    "            \"Val_Ground Truth\": val_ground_truth,\n",
    "            \"Model_Name\": model_name,\n",
    "            \"Hidden_Dim\": model.base_model.out_channels,\n",
    "            \"Periods\": model.base_model.periods,\n",
    "            \"Batch_Size\": model.base_model.batch_size,\n",
    "            \"Optimizer\": optimizer.__class__.__name__,\n",
    "            \"Learning_Rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"Weight_Decay\": optimizer.param_groups[0]['weight_decay'],\n",
    "            \"Loss_Function\": \"MSELoss\",\n",
    "            \"Epochs\": epoch,\n",
    "            \"Patience\": patience,\n",
    "            \"Train_Loss\": train_losses,\n",
    "            \"Train_Unweighted_Loss\": train_unweighted_losses,\n",
    "            \"Val_Loss\": val_losses,\n",
    "            \"Val_Unweighted_Loss\": val_unweighted_losses,\n",
    "            \"Test_Loss\": test_loss,\n",
    "            \"Test_Unweighted_Loss\": test_unweighted_loss            \n",
    "        }\n",
    "    \n",
    "    with open (f\"../Data/Results/{model_name}_results.pkl\", 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "model_evaluation(model, test_loader, test_files, model_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
